总结：

第1讲 机器学习的相关概念，sigmoid和ReLU函数，从一般的回归预测过渡到深度学习。

第2讲 

# 第一讲、机器学习概念



## 一、直观理解

机器学习≈寻找一个人类写不出来的复杂函数。比如声音识别、图像识别，人无法描述出这样的函数，所以需要机器学习完成这个任务。

## 二、深度学习

机器学习的子类，函数为类神经网络，其输入为vector，matrix（eg.图片），sequence（eg.音频）；其输出为scalar（即regression），classes（即classification，eg.是否垃圾邮件，下围棋 ），text or image。

- 有监督学习
- 自监督学习（需要pre-train）
- 无监督学习

> <font color='blue'>疑问：</font>在有监督学习时，比如我们要判断是自行车还是小汽车，它们具有明显的特征区别，人工给予label后，机器通过学习不同的特征就可以分别出来，但是宝可梦和数码宝贝也具有明显的这种区别吗？
>
> <font color='red'>解释：</font>宝可梦是动物风格，数码宝贝是机甲风格。（不过虽然如此，感觉这种区分方式还是有点模糊，而且由于设计师风格的不同，两家之间的角色差距可能十分之小。因此此处的分类感觉更偏向于将所有的现存角色输入训练后，去判断一张已被训练的角色的不同角度或不同动作的照片属于数码宝贝还是宝可梦。不知道当一个全新的角色出现时，旧有的模型是否还能进 行准确判断？

<img src="./李宏毅2022机器学习.assets/image-20240620132030716.png" alt="image-20240620132030716" style="zoom:50%;" />

异常检测和人类攻击，黑白识别出来，彩色就不行了

## 三、机器学习步骤

### （一）、Function with unknown & （二）、Define Loss from training data

如果函数为 $y=b+wx$，则Loss为 $L(b,w)$（括号里为未知参数）。比如做N次预测,可取
$$
Loss:L=\frac{1}{N}\sum_1^N e_n
$$
对于$e_n$，可以用如$MAE:e=\left\vert y-\hat{y} \right\vert$或者$MSE:e=(y-\hat{y})^2$​。

> 由于定义方式不同，Loss可以为负。<font color='blue'>疑问 </font>什么情况为负?

且可以取不同的b和w，做出一个b-w图，从而直观看出哪组值最好。

<img src="./李宏毅2022机器学习.assets/image-20240620200344416.png" alt="image-20240620200344416" style="zoom:50%;" />

### （三）、Optimization

找出最好的一组w和b。（假设w为横坐标,loss为纵坐标,只有一个参数w）(两个参数则求偏导)

**方法 梯度下降Gradient Descent** 

- （1）随机选一个值 （2）计算微分，为负则Increase w，为正则Decrease w,而增大或减小的幅度由**斜率大小**和设定的**学习率learning rate**确定。(3)更新w,直到微分为0.

  > 超参数 batch，epoch，update

  <font color='red'>疑问：</font>学习率可以设置多个吗？对每一个参数都单独设置一个学习率？

  <font color='red'>解释：</font>是的，在下文中就用到了，不同的参数设置不同的学习率。

- 缺点是找到的可能是局部最小值local minima，而不是全局最小值global minima.

## 四、更复杂的函数

Linear model不能满足所有情况，此时需要更复杂的函数，比如用piecewise linear curve（曲线可以用直线近似）叠加。



<img src="./李宏毅2022机器学习.assets/image-20240620231010649.png" alt="image-20240620231010649" style="zoom:40%;" />



而上面的蓝色曲线可用**sigmoid函数表示**

### - Sigmoid函数（<font color='blue'>重要</font> <font color='red'>学完后面再来反复理解下列过程</font>）

$$
\begin{aligned}
y&=\textcolor{red}{c}\frac{1}{1+e^{-(\textcolor{green}{b}+\textcolor{blue}{w}x)}}\\
&=\textcolor{red}{c}\  sigmoid(\textcolor{green}{b}+\textcolor{blue}{w}x)
\end{aligned}
$$

通过改变c,b,w可以得到不同的形状：

<img src="./李宏毅2022机器学习.assets/image-20240620232411534.png" alt="image-20240620232411534" style="zoom:50%;" />

上面的红色曲线，可以用**四条**（或更多）**蓝色曲线**合成（其中三个sigmoid函数，一个constant<图片中b>）
$$
y=cons+\sum_i{c_i\ sigmoid(b_i+w_i)}
$$
​                               <img src="./李宏毅2022机器学习.assets/image-20240621000021930.png" alt="image-20240621000021930" style="zoom: 40%;" />   <img src="./李宏毅2022机器学习.assets/image-20240621000209014.png" alt="image-20240621000209014" style="zoom:40%;" />

<img src="./李宏毅2022机器学习.assets/image-20240621000610213.png" alt="image-20240621000610213" style="zoom:45%;" />

### - ReLU函数

这里的sigmoid也可以换成max，也就是ReLU函数。它们都是激活函数Activation function。

<font color='red'>疑问</font>：什么是hard sigmoid？

> 上图中，可以将 $a$ 视为 $x$​​ ，再套一层，即再加一层layer。这就是神经网络Neural Network。而更多的layers意味着deep，即深度学习Deep Learning。
>
> **注意：**layers并不是越多越好，可能过拟合Overfitting(训练集上表现好，预测集上表现不好；与之相反是欠拟合Underfitting)
>
> <img src="./李宏毅2022机器学习.assets/image-20240621095015585.png" alt="image-20240621095015585" style="zoom:44%;" />

<font color='red'>疑问：</font>大概知道如何做，但不知道为什么要这样做：为什么加layer效果会更好呢？



# 第二讲、模型的优化

<img src="./李宏毅2022机器学习.assets/image-20240621111505715.png" alt="image-20240621111505715" style="zoom:50%;" />

## 一、Loss不够小

### （一）可能性一

<font color='red'>疑问：</font>什么是模型弹性？model bias是什么？

- **Model bias**：原因：model太小。解决方法：增加feature（增大弹性），从而囊括最优解（针不在海里）

- **Optimization**：原因：可能得到了局部最优而不是全局最优（针在海里但捞不到）解决方法：[见下文](#optimization优化)

  如何判断是model bias还是optimization issue？

  下图中**训练集**上56层的弹性明明比20层更大（因为多出的36层什么也不做就可以达到20层的效果），它不可能做不到20层可以做到的事，结果它的loss反而更大，说明这不是过拟合，是optimization issue。

<img src="./李宏毅2022机器学习.assets/image-20240621112312196.png" alt="image-20240621112312196" style="zoom:50%;" />

### （二）可能性二

**Overfitting**：模型的参数越多，弹性越大，越可能overfitting。判断：training data的loss小，testing data的loss大（上面第二个图的两条线交换）

- 比如训练集$(x^i,\hat y^i)$,预测 $f(x)=\begin{cases}\hat y^i &\exists x^i=x\\randown&\text{otherwise}\end{cases}$​，此时函数有 zero training loss,but large testing loss，这是一个“一无是处的模型”。

  > 注解：这门课程中正确值是$\hat y$，而预测值是$y$。可能与其他地方表示不同。

>  判断时不能一看testing data的loss大，就觉得是overfitting，还要看training data

解决办法：（1）增加训练集More training data （2）数据增强Data augmentation<比如对图像左右翻转，裁剪，**但不会上下颠倒**> （3）限制模型弹性constrained model<比如知道模型是二次函数，则不用选择更复杂的模型> 方法：更少的参数、共享参数。（4）Less features<之前用3天的数据，现在用2天的数据> （5）Early stopping （6）Regularization （7）Dropout

**注意：**不要限制太多，否则导致model bias。

**注意：**也不能只看在public testing set上的得分来选模型，不然就算一个“一无是处的模型”也可能得到非常好的结果，但是在private testing set上就不行了。理想上不用public testing set，而是在training set里面分出一部分做test。

由此引出：

## 二、Cross Validation

<img src="./李宏毅2022机器学习.assets/image-20240621125008447.png" alt="image-20240621125008447" style="zoom:50%;" />

​    分原training set为training set和validation set时，可以采用k折交叉验证k-fold Cross Validation，即将原training set分成k份，每个模型都要跑遍所有的划分情况。

## 三、<span id="optimization优化">Optimization issue的原因</span>

### （一）critical point临界点的可能情况

 1.  #### Local minima

     没办法了...（实际上很多时候并不是真的Local minima）

 2.  #### Saddle point

     可以escape，因为周围还有路，可以让loss更低。

### （二）如何判断critical point的类型

要判断，则需要知道loss function的形状，对于临界点$L(\theta)$，可以用泰勒展开来近似得到该点周围的函数表达。

<img src="./李宏毅2022机器学习.assets/image-20240621162007024.png" alt="image-20240621162007024" style="zoom:50%;" />

当在临界点时，绿色框为0：

<img src="./李宏毅2022机器学习.assets/image-20240621162155912.png" alt="image-20240621162155912" style="zoom:50%;" />

可以根据 $v^THv$ 来判断是哪种点，实际上就是判断 $H $ 的正定情况。

<img src="./李宏毅2022机器学习.assets/image-20240621162325420.png" alt="image-20240621162325420" style="zoom:50%;" />

----------

<font color='blue'>疑问1</font>：如何看出来这六个点是minima的？

<font color='red'>解释1</font>：看等高线密度。以下方三个点为例，左下侧等高线和右上侧等高线都更密，loss更大。

<font color='red'>疑问2</font>：如何看出中间的点是saddle的？

<font color='red'>解释2</font>：

<img src="./李宏毅2022机器学习.assets/image-20240621162809200.png" alt="image-20240621162809200" style="zoom:50%;" />

---

### （三）如何解决Saddle point问题（<font color='red'>重要</font>）

<img src="./李宏毅2022机器学习.assets/image-20240621171004892.png" alt="image-20240621171004892" style="zoom:50%;" />

​	注意：这里的“绿色框”表示 $\lambda$ 是 $H$ 的特征值，且与 $u$​​​ 对应。

​	注意：由于这种方法计算复杂，不常用。

**总结：[从表面上看是local minima时，上升的更高维的空间中，可能会变成saddle point，所以维度越高，可走的路就可能更多（local minima无路走，saddle point有两条路......）。](【不会还有人没听【2022】最新 李宏毅大佬的深度学习与机器学习吧？？？】 【精准空降到 29:59】 https://www.bilibili.com/video/BV1J94y1f7u5/?p=13&share_source=copy_web&vd_source=272d3cee9b462e768c4a6af6baf4564f&t=1799)**

### （四）Empirical Study

实际研究中，往往是更像local minima，而不是真的local minima。

Minimum ratio往往只会到0.5，而到1时才为真的local minima。

<img src="./李宏毅2022机器学习.assets/image-20240621173103897.png" alt="image-20240621173103897" style="zoom:50%;" />

## 三、Optimization with Batch...

### （一）Batch

<font color='purple'>update</font>时不是用整个data，而是分成多个<font color='purple'>batch</font>，依次计算每个batch的gradient进行更新，每计算完一轮，为一个<font color='purple'>epoch</font>，并且下一个epoch要重新<font color='purple'>shuffle</font>。

- 直观上以为batch size大的时候一个epoch需要的时间长



<img src="./李宏毅2022机器学习.assets/image-20240621175714073.png" alt="image-20240621175714073" style="zoom:50%;" />

- 但是实际上，由于平行计算，batch size大的时候需要时间不一定就更长，

​	<font color='red'>疑问</font>：如何进行平行计算的？

- 反而一般来说，batch size小的时候，一个epoch需要的时间反而更长（它的一个update需要的时间短）

<img src="./李宏毅2022机器学习.assets/image-20240621180720540.png" alt="image-20240621180720540" style="zoom:50%;" />

**综上，大的Batch size在时间上并不吃亏，也即不是真的“Long time”。**

**所以，Batch size越大就越好吗？并不是。**

- 因为小的batch size反而有更高的Accuracy准确度。因为它有多条路去Gradient Decent，在某一个Loss stuck的时候，下一个batch在该参数处不一定stuck。

<img src="./李宏毅2022机器学习.assets/image-20240621181601374.png" alt="image-20240621181601374" style="zoom:50%;" />

 **两者对比**

<img src="./李宏毅2022机器学习.assets/image-20240621182753637.png" alt="image-20240621182753637" style="zoom:50%;" />

### （二）Momentum

1. 梯度下降通常为普通Vanilla Gradient Descent。

   ​    每计算完一个gradient，往它的反方向去update。

2. 进阶为Vanilla Gradient+Momentum（当下的gradient+一个偏移值）

   ​    即反方向移动时加上一个movement of last step minus gradient at president。

   <img src="./李宏毅2022机器学习.assets/image-20240621183841174.png" alt="image-20240621183841174" style="zoom:50%;" />

    > 更通俗的解释：动量Momentum推动梯度下降法越过critical point。
    >
    >    <img src="./李宏毅2022机器学习.assets/image-20240621184827828.png" alt="image-20240621184827828" style="zoom:50%;" />

### （三）Learning Rate

1. #### 人工设置存在的问题

   有时候根本就到达不了critical point，因为gradient descent步子太大（learning rate较大决定步子大）每次都跨国critical point。但是learning rate太小也不行，也会走不到critical point，如右图，因为每次迈的步子太小了。<$\eta$ 为learning rate>

<img src="./李宏毅2022机器学习.assets/image-20240621191222050.png" alt="image-20240621191222050" style="zoom:50%;" />

2. #### 解决办法（<font color='red'>疑问</font>，<font color='red'>重要</font>）

   不同的参数设置不同的learning rate；<font color='red'>同一个参数的learning rate也在变化！</font>

   t表示第t次iteration迭代，i表示第i个参数。$\sigma_i^t$ 即为不同的参数设置不同的学习率。

   <img src="./李宏毅2022机器学习.assets/image-20240621192428482.png" alt="image-20240621192428482" style="zoom:50%;" />

3. #### 求$\sigma_i^t$ 的方法

   （1）均方根Root Mean Square

   <img src="./李宏毅2022机器学习.assets/image-20240621204732730.png" alt="image-20240621204732730" style="zoom:50%;" />

   并且坡度大的时候learning rate减小，坡度小的时候learning rate增大，如下图。

   <img src="./李宏毅2022机器学习.assets/image-20240621205021647.png" alt="image-20240621205021647" style="zoom:55%;" />

此外，还有另一种方法，RMSProp。

（2）RMSProp

<img src="./李宏毅2022机器学习.assets/image-20240621210957211.png" alt="image-20240621210957211" style="zoom:50%;" />

4. #### 自适应学习率存在的问题（**<font color='red'>重要，疑问</font>**）

   - learning rate可能会“爆炸”

   解决办法：Learning Rate Scheduling

   <img src="./李宏毅2022机器学习.assets/image-20240622104420234.png" alt="image-20240622104420234" style="zoom:50%;" />
   
   > Bert、Transformer都用到Warm up。

### （四）Classification issue

1. #### 引言

​	**<font color='red'>（重要）</font>**遇到分类问题时，也可以当成回归问题来做，比如有3个class(下文均以此为例），数值化为1，2，3，但这种表示class的方式有时可能有非常严重的问题！！因为数值1和2之间的距离近，这是否意味着class 1和class 2之间更相似，class 1和class 3之间不那么相似呢？在class 1，2，3表示一、二、三年级时，这种设置可能说得过去，但是如果三种类别之间并没有关系，那么这样表示就不合适了！

<img src="./李宏毅2022机器学习.assets/image-20240622111636317.png" alt="image-20240622111636317" style="zoom:50%;" />

​	那怎么办呢？利用向量！
$$
\begin{array}{cc}
\mathrm{} &\mathrm{class1} & \mathrm{class2} &\mathrm{class3}
 \\
\hat y=&\quad\begin{bmatrix}
1\\
0\\
0
\end{bmatrix} or &
\quad\begin{bmatrix}
0\\
1\\
0
\end{bmatrix} or &
\begin{bmatrix}
0\\
0\\
1
\end{bmatrix}
\end{array}
$$
​	它们之间的距离一样！！！

​	<font color='red'>疑问</font>：[之前output是数值，现在怎么变为向量呢？](【不会还有人没听【2022】最新 李宏毅大佬的深度学习与机器学习吧？？？】 【精准空降到 04:59】 https://www.bilibili.com/video/BV1J94y1f7u5/?p=16&share_source=copy_web&vd_source=272d3cee9b462e768c4a6af6baf4564f&t=299)

​	解答：output三次，这样就是一个三维向量了。

<img src="./李宏毅2022机器学习.assets/image-20240622115056947.png" alt="image-20240622115056947" style="zoom:50%;" />

**- Softmax函数**
$$
y_i'=\frac{exp(y_i)}{\sum_j exp(y_i)} \quad 且1>y_i'>0, \quad\begin{matrix} \sum_{i}y_i'=1\end{matrix}
$$
​	Softmax函数不仅进行了归一化，而且拉大了$y_i$间的差距。

​	两个class的时候可以直接Sigmoid，效果与套Softmax后一模一样。（<font color='red'>疑问</font>待推导）

2. #### Loss of Classification

   可以用MSE（$e=\sum_i(\hat y_i-y_i')^2$），更常用的是交叉熵Cross-entropy（$e=-\sum_i \hat y_ilny_i'$）(是上面一张图里Classification的$\hat y$ 和$y'$​）

   > Minimizing cross-entropy = maximizing likelihood
   >
   > pytorch用cross-entropy时，已经自动加了一层softmax了。

   注：为什么有时用Cross-entropy而不用MSE：

   <img src="./李宏毅2022机器学习.assets/image-20240622155029178.png" alt="image-20240622155029178" style="zoom:50%;" />

# 第三讲、CNN [Designed for image]

## 一、简介

<img src="./李宏毅2022机器学习.assets/image-20240622162006201.png" alt="image-20240622162006201" style="zoom:50%;" />

问：如何将图片信息输入模型？

答：假定图片的size相同，每图片由三层——RGB（即三个<font color='purple'>channel</font>）构成，将每层拉直成一个向量，三个合成一个巨长的向量直接丢进模型。但每个neuron都要看整张图片，会导致模型异常复杂，参数非常多。因此有无简化的办法呢？



### （一）简化1

**让每个neuron只用关注整张图片的一小部分（可重叠）**。



<img src="./李宏毅2022机器学习.assets/image-20240622164930783.png" alt="image-20240622164930783" style="zoom:50%;" />

注意：通常Receptive field（or kernel size）为3×3，涵盖all channels。

注意：之所以有overlap重叠，是因为如果不重叠，那么若pattern刚好在两个receptive field的边界上，则就没有neuron能够完整侦测到了；当receptive field某边空白时，可以padding，其方法有多种，可以补0、均值等。

<img src="./李宏毅2022机器学习.assets/image-20240622165920508.png" alt="image-20240622165920508" style="zoom:50%;" />

### （二）简化2

[鸟嘴](【不会还有人没听【2022】最新 李宏毅大佬的深度学习与机器学习吧？？？】 【精准空降到 20:45】 https://www.bilibili.com/video/BV1J94y1f7u5/?p=22&share_source=copy_web&vd_source=272d3cee9b462e768c4a6af6baf4564f&t=1245)可能会出现在不同的receptive field，需要让每个neuron都有侦测鸟嘴的功能吗？并不，**neuron可以共享参数**。并且由于每个neuron的输入不同，即使参数相同，输出也不会相同。

<img src="./李宏毅2022机器学习.assets/image-20240622170947407.png" alt="image-20240622170947407" style="zoom:50%;" />

这种方法实践上是怎么运用的呢？

以下图为例，假设每个receptive field有64个neuron侦测（即彩色圈圈），每个receptive field共享一组参数，即对应的neuron参数相同（红对红，黄对黄...）。

<img src="./李宏毅2022机器学习.assets/image-20240622171239742.png" alt="image-20240622171239742" style="zoom:50%;" />
